{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6c38e6",
   "metadata": {},
   "source": [
    "Теоретический материал – Нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02b9b3",
   "metadata": {},
   "source": [
    "Обучение персептрона\n",
    "Персептрон представляет собой элементарную часть нейронной сети.\n",
    "Одиночный персептрон является линейным бинарным классификатором. В\n",
    "этой лекции мы рассмотрим процедуру обучения персептрона для\n",
    "классификации данных. Поскольку персептрон представляет собой\n",
    "бинарный классификатор, то мы будем рассматривать лишь два класса.\n",
    "Пусть мы рассматриваем некоторое множество (конечное или\n",
    "бесконечное) n-мерных векторов, которые будем обозначать 𝑥 =\n",
    "(𝑥1, 𝑥2, . . . , 𝑥𝑛)\n",
    "Будем считать, что это множество разбивается на два класса, которые\n",
    "мы будем обозначать +1 и -1. Поэтому возникает задача построения\n",
    "функции, которая задана на нашем множестве векторов, и принимает\n",
    "значения в множестве {+1, −1}. В качестве такой функции может выступать\n",
    "персептрон. С алгебраической точки зрения персептрон состоит из вектора\n",
    "весов 𝑤 = (𝑤0, 𝑤1, 𝑤2, . . . , 𝑤𝑛).\n",
    "При этом персептрон работает по формуле\n",
    "𝑦 = 𝑠𝑖𝑔𝑛(𝑤0 + 𝑥1𝑤1 + 𝑥2𝑤2 + . . . + 𝑥𝑛𝑤𝑛),\n",
    "где функция 𝑠𝑖𝑔𝑛(𝑡) равна +1, если 𝑡 ≥ 0, и равна −1, если 𝑡 < 0.\n",
    "Приведем алгоритм обучения персептрона. Пусть у нас есть набор\n",
    "обучающих данных {(𝑥, 𝑑)}, где 𝑥 - это различные вектора, а 𝑑 из множества\n",
    "{+1, −1} указывает к какому классу относится наш вектор.\n",
    "1. Положим вектор весов 𝑤 равным нулю.\n",
    "2. Повторять 𝑁 раз следующие шаги:\n",
    "3. Для каждого тестового набора (𝑥, 𝑑):\n",
    "4. Вычислить 𝑦 = 𝑠𝑖𝑔𝑛[(𝑥, 𝑤)].\n",
    "5. Если 𝑦𝑑 < 0, то скорректировать веса 𝑤0 = 𝑤0 + 𝑎𝑑, 𝑤𝑖 =\n",
    "𝑤𝑖 + 𝑎𝑑𝑥𝑖\n",
    ", 𝑖 = 1,2, . . . , 𝑛.\n",
    "Описанный алгоритм довольно легко программировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe7dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139ad3b",
   "metadata": {},
   "source": [
    "1.1.1 Пример\n",
    "Задача:\n",
    "\tРассмотрим программу обучения персептрона на языке Python. Сначала рассмотрим основной класс персептрона, который умеет учиться по\n",
    "тестовым данным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f174cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, -0.1]\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# класс, который реализует перспептрон и его обучение\n",
    "class Perceptron:\n",
    "    def __init__(self,N):\n",
    "        # создать нулевые веса\n",
    "        self.w = list()\n",
    "        for i in range(N):\n",
    "            self.w.append( 0)\n",
    "    #метод для вычисления значения перспептрона\n",
    "    def calc(self,x):\n",
    "        res = 0\n",
    "        for i in range(len(self.w)):\n",
    "            res = res + self.w[i] * x[i]\n",
    "        return res\n",
    "    # пороговая функция активации перспептрона\n",
    "    def sign(self,x):\n",
    "        if self.calc(x) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    # обучение на одном примере\n",
    "    def learn(self, la, x, y):\n",
    "        #обучаем только, когда результат неверный\n",
    "        if y * self.calc(x) <= 0:\n",
    "            for i in range(len(self.w)):\n",
    "                self.w[i] = self.w[i] + la * y * x[i]\n",
    "    #обучение по всем данным Т - кортеж примеров\n",
    "    def learning(self, la, T):\n",
    "        #цикл обучения\n",
    "        for n in range(100):\n",
    "            #обучение по всем набору примеров\n",
    "            for t in T:\n",
    "                self.learn(la,t[0], t[1])\n",
    "#В строке 25 мы осуществляем корректировку весов. Посмотрим, как учится\n",
    "# и работает наш персептрон.\n",
    "#создаем класс двумерного перспетрона\n",
    "perceptron = Perceptron(2)\n",
    "la = 0.1 #\n",
    "#создаём примеры\n",
    "T = list()\n",
    "T.append([[2,1],1])\n",
    "T.append([[3,2],1])\n",
    "T.append([[4,1],1])\n",
    "T.append([[1,2],-1])\n",
    "T.append([[2,3],-1])\n",
    "T.append([[5,7],-1])\n",
    "perceptron.learning(la,T)# обучение перспетрона\n",
    "print(perceptron.w)# печатаем веса\n",
    "# проверим работу на тестовых примерах\n",
    "print(perceptron.sign([1.5, 2]))\n",
    "print(perceptron.sign([3, 1.5]))\n",
    "print(perceptron.sign([5, 1]))\n",
    "print(perceptron.sign([5, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f293f",
   "metadata": {},
   "source": [
    "Видим, что что наш персептрон отлично научился распознавать образы,\n",
    "относя к классу 1 те вектора, у которых первая компонента больше второй,\n",
    "и к классу -1 в противном случае. Хотя устройство персептронов довольно\n",
    "простое эти конструкции могут решать и практические задачи. Кроме того,\n",
    "из таких персептронов состоят нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9684995",
   "metadata": {},
   "source": [
    "Теоретический материал – Реализация нейронной сети на Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b8904cf",
   "metadata": {},
   "source": [
    "Нейронная сеть — это функциональная единица машинного или\n",
    "глубокого обучения. Она имитирует поведение человеческого мозга,\n",
    "поскольку основана на концепции биологических нейронных сетей.\n",
    "Наиболее распространенный тип нейронной сети, называемый\n",
    "многослойным персептроном (MLP), представляет собой функцию, которая\n",
    "отображает входные данные в выходные данные. MLP имеет один входной\n",
    "слой и один выходной слой. Между ними может быть один или несколько\n",
    "скрытых слоев. Входной слой имеет тот же набор нейронов, что и признаки.\n",
    "Скрытые слои также могут иметь более одного нейрона. Каждый нейрон\n",
    "представляет собой линейную функцию, к которой применяется функция\n",
    "активации для решения сложных задач. Выход каждого слоя подается в\n",
    "качестве входных данных для всех нейронов следующих слоев.\n",
    "Нейронные сети способны решать множество задач. В основном они\n",
    "состоят из таких компонентов:\n",
    "− входной слой (получение и передача данных);\n",
    "− скрытый слой (вычисление);\n",
    "− выходной слой. Чтобы реализовать нейросеть, необходимо\n",
    "понимать, как ведут себя нейроны. Нейрон одновременно\n",
    "принимает несколько входов, обрабатывает эти данные и выдает\n",
    "один выход. Нейронная сеть представляет собой блоки ввода и\n",
    "вывода, где каждое соединение имеет соответствующие веса (это\n",
    "сила связи нейронов; чем вес больше, тем один нейрон сильнее\n",
    "влияет на другой). Данные всех входов умножаются на веса:\n",
    "− 𝑥 → 𝑥 ∗ 𝑤1;\n",
    "− 𝑦 → 𝑦 ∗ 𝑤2.\n",
    "Входы после взвешивания суммируются с прибавлением значения\n",
    "порога «c»:\n",
    "𝑥𝑤1 + 𝑦𝑤2 + 𝑐\n",
    "Полученное значение пропускается через функцию активации\n",
    "(сигмоиду), которая преобразует входы в один выход:\n",
    "𝑧 = 𝑓(𝑥𝑤1 + 𝑦𝑤2 + 𝑐).\n",
    "Так выглядит сигмоида:\n",
    "\n",
    "Интервал результатов сигмоиды — от 0 до 1. Отрицательные числа\n",
    "стремятся к нулю, а положительные — к единице.\n",
    "Например. Пусть нейрон имеет следующие значения: 𝑤 = [0,1] 𝑐 = 4.\n",
    "Входной слой: 𝑥 = 2, 𝑦 = 3.\n",
    "((𝑥𝑤1) + (𝑦𝑤2)) + 𝑐 = 20 + 31 + 4 = 7.\n",
    "𝑧 = 𝑓(7) = 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860b519",
   "metadata": {},
   "source": [
    "1.1.2 Пример\n",
    "Решение:\n",
    "Для написания кода нейрона будем использовать библиотеку Pytnon\n",
    "— NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "057c127b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Neuron() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m weights \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m]) \n\u001B[0;32m     14\u001B[0m bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m \n\u001B[1;32m---> 15\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[43mNeuron\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m]) \n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(n\u001B[38;5;241m.\u001B[39mfeedforward(x))\n",
      "\u001B[1;31mTypeError\u001B[0m: Neuron() takes no arguments"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    # Функция активации: f(x) = 1 / (1 + e*(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "class Neuron:\n",
    "    def _init_(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) \n",
    "bias = 4 \n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2, 3]) \n",
    "print(n.feedforward(x))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0a012",
   "metadata": {},
   "source": [
    "Нейросеть состоит из множества соединенных между собой нейронов.\n",
    "Пример несложной нейронной сети\n",
    "где:\n",
    "𝑥1, 𝑥2 — входной слой;\n",
    "ℎ1, ℎ2 — скрытый слой с двумя нейронами;\n",
    "𝑜1 — выходной слой.\n",
    "Например. Представим, что нейроны из графика выше имеют веса [0, 1]. Пороговое значение (𝑏) у обоих нейронов равно 0 и они имеют идентичную сигмоиду.\n",
    "При входных данных 𝑥 = [2, 3] получим:\n",
    "ℎ1 = ℎ2 = 𝑓(𝑤𝑥 + 𝑏) = 𝑓((02) + (1 ∗ 3) + 0) = 𝑓(3) = 0.95.\n",
    "𝑜1 = 𝑓(𝑤 ∗ [ℎ1, ℎ2] + 𝑏) = 𝑓((0ℎ1) + (1ℎ2) + 0) = 𝑓(0.95) = 0.72.\n",
    "Входные данные по нейронам передаются до тех пор, пока не получатся выходные значения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8edcdbba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OurNeuralNetwork' object has no attribute 'h1'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     19\u001B[0m network \u001B[38;5;241m=\u001B[39m OurNeuralNetwork()\n\u001B[0;32m     20\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m])\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeedforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36mOurNeuralNetwork.feedforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeedforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 13\u001B[0m     out_h1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mh1\u001B[49m\u001B[38;5;241m.\u001B[39mfeedforward(x)\n\u001B[0;32m     14\u001B[0m     out_h2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh2\u001B[38;5;241m.\u001B[39mfeedforward(x)\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# Bxodu Ona 01 — 3mo Gaxody h1 u h2\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'OurNeuralNetwork' object has no attribute 'h1'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class OurNeuralNetwork:\n",
    "    \n",
    "   \n",
    "    def __init_(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "        # Knacc Neuron u3 npedwdyyezo pasdena\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        # Bxodu Ona 01 — 3mo Gaxody h1 u h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd1330",
   "metadata": {},
   "source": [
    "Теоретический материал – Обучение нейронной сети\n",
    "Обучение нейросети — это подбор весов, которые соответствуют всем входам для решения поставленных задач.\n",
    "Класс нейронной сети:\n",
    "class NeuralNetwork:\n",
    "    def __init(self, x, y):\n",
    "        self.input = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y        = y\n",
    "        self.output   = np.zeros(y.shape)\n",
    "Каждый этап процесса обучения состоит из:\n",
    "\n",
    "\tпрямого распространения (прогнозируемый выход);\n",
    "\tобратного распространения (обновление весов и смещений). Например:\n",
    "Дана двуслойная нейросеть:\n",
    "\n",
    "ŷ = 𝜎(𝑤2𝜎(𝑤1𝑥 + 𝑏1) + 𝑏2).\n",
    "\n",
    "В данном случае на выход ŷ влияют только две переменные — 𝑤 (веса) и 𝑏 (смещение). Настройку весов и смещений из данных входа или процесс обучения нейросети можно изобразить так:\n",
    "Прямое распространение.\n",
    "Как видно, формула прямого распространения представляет собой несложное вычисление:\n",
    "\n",
    "ŷ = 𝜎(𝑤2𝜎(𝑤1𝑥 + 𝑏1) + 𝑏2)\n",
    "\n",
    "Далее необходимо добавить в код функцию прямого распространения.\n",
    "Предположим, что смещения в этом случае будут равны 0.\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input =x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y        = y\n",
    "        self.output   = np.zeros(self.y.shape)\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.later1, self.weights2))\n",
    "Чтобы вычислить ошибку прогноза, необходимо использовать функцию потери. В примере уместно воспользоваться формулой суммы квадратов ошибок — средним значением между прогнозируемым и фактическим результатами:\n",
    "                        \n",
    "𝑛\n",
    "𝐸𝑟𝑟𝑜𝑟 =  ∑(𝑦 − 𝑦̂)2.\n",
    "𝑖=1\n",
    "Обратное распространение\n",
    "Обратное распространение позволяет измерить производные в обратном порядке — от конца к началу, и скорректировать веса и смещения. Для этого необходимо узнать производную функции потери — тангенс угла наклона.\n",
    "Производная функции по отношению к весам и смещениям позволяет узнать градиентный спуск. Производная функции потери не содержит весов и смещений, для ее вычисления необходимо добавить правило цепи:\n",
    "𝑛\n",
    "𝐿𝑜𝑠𝑠 (𝑦, 𝑦̂) = ∑(𝑦 − 𝑦̂)2\n",
    "𝑖=1\n",
    "𝜕𝐿𝑜𝑠𝑠 (𝑦, 𝑦̂)\t𝜕𝐿𝑜𝑠𝑠 (𝑦, 𝑦̂)   𝜕𝑦̂\t𝜕𝑧\n",
    "𝜕𝑊\t=\t𝜕𝑦̂\t∙ 𝜕𝑧 ∙ 𝜕𝑊 =\n",
    "= 2(𝑦 − 𝑦̂) ∙ производную сигмоиды  ∙ 𝑥 =\n",
    "= 2(𝑦 − 𝑦̂) ∙ 𝑧(1 − 𝑧) ∙ 𝑥,\n",
    "где 𝑧 = 𝑊𝑥 + 𝑏.\n",
    "Благодаря этому правилу можно регулировать веса. Добавляем в код Python функцию обратного распространения:\n",
    "class NeuralNetwork:\n",
    "def ＿init＿(self, x, y):\n",
    "self.input\n",
    "self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "self.weights2 = np.random.rand(4,1)\n",
    "self.y\n",
    "self.output =np.zeros(self.y.shape)\n",
    "def feedforward(self): self.layer1 = sigmoid(np.dot(self.input, self. weights1))\n",
    "self.output = sigmoid(np.dot(self.layer1, self.weights2)) def backprop(self):\n",
    "#применение правила цепи для нахождения производной функции потерь по весу2 и весу1 \n",
    "d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output))) d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self. weights2.T) * sigmoid_derivative(self. layer1)))\n",
    "#обновление веса производной (наклона) функции потерь self.weights1 += d_weights1 self.weights2 += d_weights2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5d31f",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b7ef5",
   "metadata": {},
   "source": [
    "Данные нейросети:\n",
    "- три входа (𝑥1, 𝑥2, 𝑥3);\n",
    "- три нейрона в скрытых слоях (h1, h2, h3);\n",
    "- выход (𝑜1).\n",
    "\n",
    "Нейроны имеют идентичные веса и пороги:\n",
    "\n",
    "- 𝑤 = [0.5, 0.5, 0.5]\n",
    "- 𝑏=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb96a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f205ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron1:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward (self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f20f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8151036049051821\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5, 0.5, 0.5])\n",
    "        bias = 0 \n",
    "        self.h1 = Neuron1(weights, bias)\n",
    "        self.h2 = Neuron1(weights, bias)\n",
    "        self.h3 = Neuron1(weights, bias)\n",
    "        self.o1 = Neuron1(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "    \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3, 4])\n",
    "print (network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80615d",
   "metadata": {},
   "source": [
    "Данные нейросети:\n",
    "- два входа (𝑥1, 𝑥2);\n",
    "- два нейрона в скрытых слоях (h1, h2);\n",
    "- два выхода(𝑜1,𝑜2).\n",
    "\n",
    "Нейроны имеют идентичные веса и пороги: \n",
    "\n",
    "- 𝑤 = [1,0];\n",
    "- 𝑏 = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8511f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8757270529783324, 0.8757270529783324)\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([1,0])\n",
    "        bias = 1\n",
    "        \n",
    "        self.h1 = Neuron1(weights, bias)\n",
    "        self.h2 = Neuron1(weights, bias)\n",
    "        self.o1 = Neuron1(weights, bias)\n",
    "        self.o2 = Neuron1(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "    \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3])\n",
    "print (network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dda59",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Sigmoid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8151036049051821\n",
      "(0.8757270529783324, 0.8757270529783324)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    z = np.exp(-x)\n",
    "    sig = 1 / (1 + z)\n",
    "    return sig\n",
    "\n",
    "class Neuron1:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward (self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5, 0.5, 0.5])\n",
    "        bias = 0\n",
    "        self.h1 = Neuron1(weights, bias)\n",
    "        self.h2 = Neuron1(weights, bias)\n",
    "        self.h3 = Neuron1(weights, bias)\n",
    "        self.o1 = Neuron1(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "class OrNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([1,0])\n",
    "        bias = 1\n",
    "\n",
    "        self.h1 = Neuron1(weights, bias)\n",
    "        self.h2 = Neuron1(weights, bias)\n",
    "        self.o1 = Neuron1(weights, bias)\n",
    "        self.o2 = Neuron1(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3, 4])\n",
    "print (network.feedforward(x))\n",
    "\n",
    "network = OrNeuralNetwork()\n",
    "x = np.array ([2, 3])\n",
    "print (network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "5b66bd97",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f694f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee9fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron2:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward (self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return tanh(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66afd1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7968426715486405\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5, 0.5, 0.5])\n",
    "        bias = 0 \n",
    "        self.h1 = Neuron2(weights, bias)\n",
    "        self.h2 = Neuron2(weights, bias)\n",
    "        self.h3 = Neuron2(weights, bias)\n",
    "        self.o1 = Neuron2(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "    \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3, 4])\n",
    "print (network.feedforward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27361403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1555911185916798, 1.1555911185916798)\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([1,0])\n",
    "        bias = 1\n",
    "        \n",
    "        self.h1 = Neuron2(weights, bias)\n",
    "        self.h2 = Neuron2(weights, bias)\n",
    "        self.o1 = Neuron2(weights, bias)\n",
    "        self.o2 = Neuron2(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "    \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3])\n",
    "print (network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeade56",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82da6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "814b65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron3:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward (self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return ReLU(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b04e250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.75\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5, 0.5, 0.5])\n",
    "        bias = 0 \n",
    "        self.h1 = Neuron3(weights, bias)\n",
    "        self.h2 = Neuron3(weights, bias)\n",
    "        self.h3 = Neuron3(weights, bias)\n",
    "        self.o1 = Neuron3(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "    \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3, 4])\n",
    "print (network.feedforward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073eaa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        weights = np.array([1,0])\n",
    "        bias = 1\n",
    "        \n",
    "        self.h1 = Neuron3(weights, bias)\n",
    "        self.h2 = Neuron3(weights, bias)\n",
    "        self.o1 = Neuron3(weights, bias)\n",
    "        self.o2 = Neuron3(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "    \n",
    "network = OurNeuralNetwork()\n",
    "x = np.array ([2, 3])\n",
    "print (network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3717e",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d562032",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c946917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:  (150, 4) (150,)\n",
      "(120, 4) (30, 4) (120,) (30,)\n",
      "['Versicolor' 'Setosa' 'Virginica' 'Virginica' 'Setosa' 'Setosa'\n",
      " 'Virginica' 'Virginica' 'Virginica' 'Setosa' 'Setosa' 'Versicolor'\n",
      " 'Virginica' 'Versicolor' 'Virginica']\n",
      "80     Versicolor\n",
      "45         Setosa\n",
      "144     Virginica\n",
      "110     Virginica\n",
      "38         Setosa\n",
      "2          Setosa\n",
      "135     Virginica\n",
      "72     Versicolor\n",
      "138     Virginica\n",
      "34         Setosa\n",
      "19         Setosa\n",
      "77     Versicolor\n",
      "101     Virginica\n",
      "63     Versicolor\n",
      "117     Virginica\n",
      "Name: target, dtype: object\n",
      "Test Accuracy: 0.933\n",
      "Training Accuracy: 0.983\n",
      "Loss:  0.2988789340197434\n",
      "Number of Coefs:  2\n",
      "Number of Intercepts:  2\n",
      "Number of Iteration for Which Estimator Ran:  200\n",
      "Name of Output Layer Activation Function:  softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\иван\\pycharmprojects\\pythonproject3\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)\n",
    "df\n",
    "df = df.rename(columns={'variety': 'target'})\n",
    "X_df, Y_df = df.drop(['target'], axis=1), df.target\n",
    "print('Dataset Size: ', X_df.shape, Y_df.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size=0.80, test_size=0.20, stratify=Y_df,\n",
    "                                                    random_state=123)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "mlp_classifier.fit(X_train, Y_train)\n",
    "Y_preds = mlp_classifier.predict(X_test)\n",
    "\n",
    "print(Y_preds[:15])\n",
    "print(Y_test[:15])\n",
    "print('Test Accuracy: %.3f' % mlp_classifier.score(X_test, Y_test))\n",
    "print('Training Accuracy: %.3f' % mlp_classifier.score(X_train, Y_train))\n",
    "\n",
    "print('Loss: ', mlp_classifier.loss_)\n",
    "print('Number of Coefs: ', len(mlp_classifier.coefs_))\n",
    "print('Number of Intercepts: ', len(mlp_classifier.intercepts_))\n",
    "print('Number of Iteration for Which Estimator Ran: ', mlp_classifier.n_iter_)\n",
    "print('Name of Output Layer Activation Function: ', mlp_classifier.out_activation_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43d567",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90505246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearsExperience   Salary\n",
       "0              1.1  39343.0\n",
       "1              1.3  46205.0\n",
       "2              1.5  37731.0\n",
       "3              2.0  43525.0\n",
       "4              2.2  39891.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8fefbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'Salary':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "834bfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, Y_df = df.drop(['target'], axis=1), df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32f0b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:  (30, 1) (30,)\n"
     ]
    }
   ],
   "source": [
    "print ('Dataset Size: ', X_df.shape, Y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05ca0dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test size:  (24, 1) (6, 1) (24,) (6,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size = 0.80, test_size = 0.20, random_state = 123)\n",
    "print ('Train/Test size: ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e75bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(random_state=123)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regressor = MLPRegressor(random_state=123)\n",
    "mlp_regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75794970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.26234628 55.2781752  18.82135812 50.48274487 20.26234628 50.9622879 ]\n",
      "7      54445.0\n",
      "29    121872.0\n",
      "5      56642.0\n",
      "26    116969.0\n",
      "8      64445.0\n",
      "27    112635.0\n",
      "Name: target, dtype: float64\n",
      "Test R^2 Score: -8.796\n",
      "Training R^2 Score: -8.261\n"
     ]
    }
   ],
   "source": [
    "Y_preds = mlp_regressor.predict(X_test)\n",
    "\n",
    "print (Y_preds[:10])\n",
    "print (Y_test[:10])\n",
    "print ('Test R^2 Score: %.3f'%mlp_regressor.score(X_test, Y_test))\n",
    "print ('Training R^2 Score: %.3f'%mlp_regressor.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36813159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2988058032.1601596\n",
      "Number of Coefs:  2\n",
      "Number of Intercepts:  2\n",
      "Number of Iteration for Which Estimator Ran:  200\n",
      "Name of Output Layer Activation Function:  identity\n"
     ]
    }
   ],
   "source": [
    "print ('Loss: ', mlp_regressor.loss_)\n",
    "print ('Number of Coefs: ', len(mlp_regressor.coefs_))\n",
    "print ('Number of Intercepts: ', len(mlp_regressor.intercepts_))\n",
    "print ('Number of Iteration for Which Estimator Ran: ', mlp_regressor.n_iter_)\n",
    "print ('Name of Output Layer Activation Function: ', mlp_regressor.out_activation_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
